from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import boto3
import os
import json

def fetch_and_store_to_s3(templates_dict, **kwargs):
    api_key = ""
    aws_access_key = ""
    aws_secret_key = ""
    aws_region = "us-east-2"
    #ì½”ë“œ ì¶”ê°€
    yyyymmdd = templates_dict['api_date']
    url = f"http://openapi.seoul.go.kr:8088/{api_key}/json/CardSubwayStatsNew/1/1000/{yyyymmdd}"
    print(url)

    try:
        response = requests.get(url)
        res = response.json()['CardSubwayStatsNew'] # ['CardSubwayStatsNew']
    except Exception as e:
        print(e)
        raise Exception(e)
        return ""

    if "ERROR" in res['RESULT']['CODE']:
        print(res['RESULT']['MESSAGE'])
        return ""

    print(res['RESULT']['MESSAGE'])
    print(res['row'])

    contents = res['row']
    # file_name = f"{yyyymmdd}/{yyyymmdd}.json"
    yymmdd = yyyymmdd[2:]
    file_name = f"{yymmdd}.json"
    os.makedirs(os.path.join("/tmp", f"{yyyymmdd}"), exist_ok=True)
    full_path = os.path.join("/tmp", f"{yyyymmdd}", file_name)

    try:
        with open(full_path, "w", encoding="utf-8") as f:
            json_str = json.dumps(contents, ensure_ascii=False) #, separators=(",", ":"))
            json_str = json_str.replace("[", "").replace("]", "").replace("},","}").replace("}","}\n") #.replace("{", "\t{")
            f.write(json_str)
    except Exception as e:
        print(f"ERROR: {file_name} íŒŒì¼ ì €ìž¥ ì‹¤íŒ¨")
        raise Exception(e)
        return ""
        
    print(f"{file_name} íŒŒì¼ ì €ìž¥ ì™„ë£Œ")
    
    s3_client = boto3.client("s3",
                             aws_access_key_id=aws_access_key,
                             aws_secret_access_key=aws_secret_key,
                             region_name=aws_region
                             )
    try:
        bucket_name = f"subway-assignment-daewoon"
        object_name = f"subway/{yyyymmdd}/{file_name}"
        s3_client.upload_file(full_path, bucket_name, object_name)
    except Exception as e:
        print("ERROR: AWS ìžê²© ì¦ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", e)
        raise Exception(e)

    print(f"{file_name} íŒŒì¼ s3 ì—…ë¡œë“œ ì™„ë£Œ")

def analyze_and_store_to_s3(templates_dict, **kwargs):
    # ðŸŽ¯ ë¬¸ì œ 4: S3ì—ì„œ ì›ë³¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (7ì¼ ì „ ê¸°ì¤€)
    import boto3
    import json
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import explode
    from pyspark.sql.functions import col

    # print(f"hello, analyze_and_store_to_s3")

    yyyymmdd = templates_dict['yyyymmdd']
    yymmdd = templates_dict['yymmdd']

    aws_access_key = ""
    aws_secret_key = ""
    aws_region = "us-east-2"
    s3_bucket = "subway-assignment-daewoon"
    s3_input_key = f"subway/{yyyymmdd}/{yymmdd}.json"
    s3_output_key = f"subway/analysis/{yyyymmdd}.json"
    local_input_path = f"/tmp/{yymmdd}.json"

    try:
        s3_client = boto3.client("s3",
                                aws_access_key_id=aws_access_key,
                                aws_secret_access_key=aws_secret_key,
                                region_name=aws_region
                                )
        s3_client.download_file(s3_bucket, s3_input_key, local_input_path)
    except Exception as e:
        print(f"ERROR: ", e)
        raise(e)
        return ""
    
    print(f"[INFO] S3 JSON downloaded â†’ {local_input_path}")

    try:
        spark = (
            SparkSession.builder
            .appName("subway-analysis")
            # # .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262")
            # .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.4.0,com.amazonaws:aws-java-sdk-bundle:1.12.664")
            # .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
            # .config("spark.hadoop.fs.s3a.access.key", aws_access_key)
            # .config("spark.hadoop.fs.s3a.secret.key", aws_secret_key)
            # .config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com")
            .getOrCreate()
        )
    except Exception as e:
        print(f"ERROR: Spark Session ì´ˆê¸°í™” ì‹¤íŒ¨")
        raise Exception(e)
        return ""

    print(f"Spark Version: {spark.version}")
    print(f"Spark Session ì´ˆê¸°í™” ì„±ê³µ")

    try:
        df = spark.read.json(local_input_path)
        print("[INFO] ë¡œì»¬ JSON â†’ DataFrame ë¡œë“œ ì„±ê³µ")
        df.show(truncate=False)
    except Exception as e:
        print(f"[ERROR] JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
        spark.stop()
        raise
    
    try:
        # df.createOrReplaceTempView("subway_data")

        df_casted = df.withColumn("GTON_TNOPE", col("GTON_TNOPE").cast("long")) \
                    .withColumn("GTOFF_TNOPE", col("GTOFF_TNOPE").cast("long"))
        df_casted.createOrReplaceTempView("subway_data")

        sql_result = spark.sql("""
            SELECT * FROM subway_data
        """)
        print("=== SQL ê²°ê³¼ ===")
        sql_result.show(truncate=False)
    except Exception as e:
        print(f"[ERROR] SQL ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        spark.stop()
        raise

    try:
        df = df_casted

        print("\n=== [ë°ì´í„° ìŠ¤í‚¤ë§ˆ] ===")
        df.printSchema()

        # âœ… 3. ë…¸ì„ ë³„ ì´ ì´ìš©ìž ìˆ˜ (ìŠ¹ì°¨+í•˜ì°¨)
        total_by_line = spark.sql("""
            SELECT 
                SBWY_ROUT_LN_NM AS line_name,
                SUM(GTON_TNOPE + GTOFF_TNOPE) AS total_passengers
            FROM subway_data
            GROUP BY SBWY_ROUT_LN_NM
            ORDER BY total_passengers DESC
        """)
        total_by_line.show()

        # âœ… 4. ì—­ë³„ ìŠ¹ì°¨/í•˜ì°¨ ìˆ˜
        station_stats = spark.sql("""
            SELECT 
                SBWY_STNS_NM AS station_name,
                SUM(GTON_TNOPE) AS total_boarding,
                SUM(GTOFF_TNOPE) AS total_alighting
            FROM subway_data
            GROUP BY SBWY_STNS_NM
            ORDER BY total_boarding DESC
        """)
        station_stats.show(10)

        # âœ… 5. ìŠ¹í•˜ì°¨ ë¹„ìœ¨ ìƒìœ„/í•˜ìœ„
        ratio_df = spark.sql("""
            SELECT 
                SBWY_STNS_NM AS station_name,
                ROUND(SUM(GTON_TNOPE) / NULLIF(SUM(GTOFF_TNOPE), 0), 2) AS board_to_alight_ratio
            FROM subway_data
            GROUP BY SBWY_STNS_NM
            ORDER BY board_to_alight_ratio DESC
        """)
        ratio_df.show(10)

        # âœ… 6. TOP 5 ì´ìš©ì—­
        top5_stations = spark.sql("""
            SELECT 
                SBWY_STNS_NM AS station_name,
                SUM(GTON_TNOPE + GTOFF_TNOPE) AS total_usage
            FROM subway_data
            GROUP BY SBWY_STNS_NM
            ORDER BY total_usage DESC
            LIMIT 5
        """)
        top5_stations.show()

        # âœ… 7. ê²°ê³¼ë¥¼ Python dictë¡œ ë³€í™˜
        result = {
            "total_by_line": [r.asDict() for r in total_by_line.collect()],
            "station_stats": [r.asDict() for r in station_stats.collect()],
            "ratio_top10": [r.asDict() for r in ratio_df.limit(10).collect()],
            "top5_stations": [r.asDict() for r in top5_stations.collect()],
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),

            # "total_by_line": [row.asDict() for row in total_by_line.collect()],
            # "station_stats": [row.asDict() for row in station_stats.collect()],
            # "ratio_top10": [row.asDict() for row in ratio_df.limit(10).collect()],
            # "top5_stations": [row.asDict() for row in top5_stations.collect()],
        }

        print("\nâœ… ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(result)

    except Exception as e:
        print("\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
        # print(traceback.format_exc())
        spark.stop()
        raise e

    try:
        # âœ… 5ï¸âƒ£ dict â†’ JSON ë¬¸ìžì—´ (UTF-8, í•œê¸€ ê¹¨ì§ ë°©ì§€)
        json_str = json.dumps(result, ensure_ascii=False, indent=2)

        # âœ… 8ï¸âƒ£ ë©”ëª¨ë¦¬ ë‚´ ë¬¸ìžì—´ ë°”ë¡œ ì—…`ë¡œë“œ
        s3_client.put_object(
            Bucket=s3_bucket,
            Key=s3_output_key,
            Body=json_str.encode("utf-8"),
            ContentType="application/json",
        )

        print(f"\nâœ… ë¶„ì„ ê²°ê³¼ê°€ S3ì— ì„±ê³µì ìœ¼ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"â†’ s3://{s3_bucket}/{s3_output_key}")
    except Exception as e:
        print("\n ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
        spark.stop()
        raise(e)

    spark.stop()
    # s3_full_path = f"s3a://{s3_bucket}/{s3_input_key}"
    # try:

    #     df = spark.read.json(s3_full_path)
    #     df.show(10, truncate=False)
    # except Exception as e:
    #     print(f"ERROR: {e}")
    #     raise(e)
    #     return ""
    # print(f"s3 read ì„±ê³µ")


    # try:
    #     with open(local_input_path, "w", encoding="utf-8") as f:
    #         f.write(df)
    # except Exception as e:
    #     print(f"ERROR: {file_name} íŒŒì¼ ì €ìž¥ ì‹¤íŒ¨")
    #     raise Exception(e)
    #     return ""
        
    # print(f"{file_name} íŒŒì¼ ì €ìž¥ ì™„ë£Œ")


   #ì½”ë“œ ì¶”ê°€

   
# -------------------------
# DAG ì •ì˜ ë° Task êµ¬ì„±
# -------------------------

with DAG(
    dag_id='fetch_subway_data_with_yymmdd_filename',
    # default_args=default_args,
    schedule='@daily',
    catchup=False, # True,
    tags=['subway', 's3', 'api'],
    description='ì„œìš¸ì‹œ ì§€í•˜ì²  ë°ì´í„°ë¥¼ yymmdd í˜•ì‹ íŒŒì¼ë¡œ S3ì— ì €ìž¥',
) as dag:

    fetch_task = PythonOperator(
        task_id='fetch_and_store_to_s3',
        python_callable=fetch_and_store_to_s3,
        templates_dict={
            'api_date': "{{ macros.ds_format(macros.ds_add(ds, -7), '%Y-%m-%d', '%Y%m%d') }}",
            'yyyymmdd': "{{ macros.ds_format(macros.ds_add(ds, -7), '%Y-%m-%d', '%Y%m%d') }}",
            'yymmdd': "{{ macros.ds_format(macros.ds_add(ds, -7), '%Y-%m-%d', '%y%m%d') }}"
        }
    )

    analyze_task = PythonOperator(
        task_id='analyze_and_store_to_s3',
        python_callable=analyze_and_store_to_s3,
        templates_dict={
            'yyyymmdd': "{{ macros.ds_format(macros.ds_add(ds, -7), '%Y-%m-%d', '%Y%m%d') }}",
            'yymmdd': "{{ macros.ds_format(macros.ds_add(ds, -7), '%Y-%m-%d', '%y%m%d') }}"
        }
    )

    # Task ì—°ê²°
    fetch_task >> analyze_task
